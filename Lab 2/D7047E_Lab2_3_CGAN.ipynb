{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oWA6qFFUIt1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "# Load MNIST dataset\n",
        "\n",
        "mnist_data = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist_data.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices (one-hot encoding)\n",
        "num_classes = 10 # There are 10 classes in MNIST\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "mb_size = 64\n",
        "Z_dim = 100\n",
        "X_dim = 28*28\n",
        "y_dim = num_classes\n",
        "h_dim = 128\n",
        "cnt = 0\n",
        "lr = 1e-3"
      ],
      "metadata": {
        "id": "aYdhh7MxJHef"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
      ],
      "metadata": {
        "id": "F5388LURJ8n6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
        "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
        "\n",
        "\n",
        "def G(z, c):\n",
        "    inputs = torch.cat([z, c], 1)\n",
        "    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
        "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X\n",
        "\n",
        "\n",
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
        "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "\n",
        "def D(X, c):\n",
        "    inputs = torch.cat([X, c], 1)\n",
        "    h = nn.relu(inputs @ Wxh + bxh.repeat(inputs.size(0), 1))\n",
        "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DQR3uhszJ9qb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
        "\n",
        "\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
        "\n",
        "\n",
        "G_solver = optim.Adam(G_params, lr=1e-3)\n",
        "D_solver = optim.Adam(D_params, lr=1e-3)\n",
        "\n",
        "ones_label = Variable(torch.ones(mb_size, 1))\n",
        "zeros_label = Variable(torch.zeros(mb_size, 1))\n",
        "\n",
        "\n",
        "for it in range(100000):\n",
        "    # Sample data\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    idx = np.random.randint(0, X_train.shape[0], mb_size)\n",
        "    X = Variable(torch.from_numpy(X_train[idx]).float())\n",
        "    c = Variable(torch.from_numpy(y_train_one_hot[idx]).float())\n",
        "\n",
        "    # Dicriminator forward-loss-backward-update\n",
        "    G_sample = G(z, c)\n",
        "    D_real = D(X, c)\n",
        "    D_fake = D(G_sample, c)\n",
        "\n",
        "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
        "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    G_sample = G(z, c)\n",
        "    D_fake = D(G_sample, c)\n",
        "\n",
        "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
        "\n",
        "        c = np.zeros(shape=[mb_size, y_dim], dtype='float32')\n",
        "        c[:, np.random.randint(0, 10)] = 1.\n",
        "        c = Variable(torch.from_numpy(c))\n",
        "        samples = G(z, c).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(cnt).zfill(3)), bbox_inches='tight')\n",
        "        cnt += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jcrfCyVKNpr",
        "outputId": "efac4c66-8f22-4d16-e9c6-5c898e98cbee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 0.005447774659842253; G_loss: 6.082343578338623\n",
            "Iter-1000; D_loss: 0.0003743061679415405; G_loss: 10.756275177001953\n",
            "Iter-2000; D_loss: 0.0024052676744759083; G_loss: 9.610559463500977\n",
            "Iter-3000; D_loss: 0.001010522129945457; G_loss: 9.340376853942871\n",
            "Iter-4000; D_loss: 0.009231614880263805; G_loss: 10.579803466796875\n",
            "Iter-5000; D_loss: 0.11352275311946869; G_loss: 8.274649620056152\n",
            "Iter-6000; D_loss: 0.01767956092953682; G_loss: 7.710787773132324\n",
            "Iter-7000; D_loss: 0.34597349166870117; G_loss: 5.000637054443359\n",
            "Iter-8000; D_loss: 0.3477376699447632; G_loss: 4.466437339782715\n",
            "Iter-9000; D_loss: 0.29114729166030884; G_loss: 4.2505083084106445\n",
            "Iter-10000; D_loss: 0.7813863754272461; G_loss: 3.4069857597351074\n",
            "Iter-11000; D_loss: 0.5685049295425415; G_loss: 3.1460330486297607\n",
            "Iter-12000; D_loss: 0.8054193258285522; G_loss: 2.4995107650756836\n",
            "Iter-13000; D_loss: 1.0170000791549683; G_loss: 2.715989351272583\n",
            "Iter-14000; D_loss: 0.8165727853775024; G_loss: 2.09181809425354\n",
            "Iter-15000; D_loss: 0.9864166378974915; G_loss: 2.1359140872955322\n",
            "Iter-16000; D_loss: 0.7760810852050781; G_loss: 1.9652113914489746\n",
            "Iter-17000; D_loss: 0.8473856449127197; G_loss: 2.248699188232422\n",
            "Iter-18000; D_loss: 0.7048734426498413; G_loss: 2.196560859680176\n",
            "Iter-19000; D_loss: 0.9451771974563599; G_loss: 1.7897734642028809\n",
            "Iter-20000; D_loss: 0.8389474153518677; G_loss: 1.5368715524673462\n",
            "Iter-21000; D_loss: 1.1092658042907715; G_loss: 1.9397403001785278\n",
            "Iter-22000; D_loss: 1.002301812171936; G_loss: 1.793962001800537\n",
            "Iter-23000; D_loss: 0.770844578742981; G_loss: 2.0053462982177734\n",
            "Iter-24000; D_loss: 0.7551511526107788; G_loss: 1.9134858846664429\n",
            "Iter-25000; D_loss: 0.7609366774559021; G_loss: 2.1106772422790527\n",
            "Iter-26000; D_loss: 0.9435797929763794; G_loss: 1.6742886304855347\n",
            "Iter-27000; D_loss: 0.7374621629714966; G_loss: 1.4718687534332275\n",
            "Iter-28000; D_loss: 0.8612349033355713; G_loss: 1.6208852529525757\n",
            "Iter-29000; D_loss: 0.8178846836090088; G_loss: 1.7415313720703125\n",
            "Iter-30000; D_loss: 0.5953158140182495; G_loss: 1.6287246942520142\n",
            "Iter-31000; D_loss: 0.7324814796447754; G_loss: 2.090435028076172\n",
            "Iter-32000; D_loss: 0.730621337890625; G_loss: 2.176048755645752\n",
            "Iter-33000; D_loss: 0.8141119480133057; G_loss: 2.106653928756714\n",
            "Iter-34000; D_loss: 0.7393598556518555; G_loss: 1.7413365840911865\n",
            "Iter-35000; D_loss: 0.9347869157791138; G_loss: 1.9630274772644043\n",
            "Iter-36000; D_loss: 0.8818967342376709; G_loss: 1.8021214008331299\n",
            "Iter-37000; D_loss: 0.9055761694908142; G_loss: 2.0637738704681396\n",
            "Iter-38000; D_loss: 0.5868822336196899; G_loss: 2.221961498260498\n",
            "Iter-39000; D_loss: 0.6958440542221069; G_loss: 1.8198802471160889\n",
            "Iter-40000; D_loss: 0.6329707503318787; G_loss: 1.910090684890747\n",
            "Iter-41000; D_loss: 0.8731088638305664; G_loss: 2.153892993927002\n",
            "Iter-42000; D_loss: 0.6763181090354919; G_loss: 1.8422210216522217\n",
            "Iter-43000; D_loss: 0.774702787399292; G_loss: 1.9565280675888062\n",
            "Iter-44000; D_loss: 0.8158739805221558; G_loss: 2.006802797317505\n",
            "Iter-45000; D_loss: 0.9790160655975342; G_loss: 1.8731001615524292\n",
            "Iter-46000; D_loss: 0.7435050010681152; G_loss: 2.0076904296875\n",
            "Iter-47000; D_loss: 0.8932693600654602; G_loss: 1.9246151447296143\n",
            "Iter-48000; D_loss: 0.9524400234222412; G_loss: 1.7953763008117676\n",
            "Iter-49000; D_loss: 0.7457028031349182; G_loss: 1.9991610050201416\n",
            "Iter-50000; D_loss: 0.8220285773277283; G_loss: 1.8277591466903687\n",
            "Iter-51000; D_loss: 0.5952783823013306; G_loss: 2.057703971862793\n",
            "Iter-52000; D_loss: 0.8313549757003784; G_loss: 1.8150403499603271\n",
            "Iter-53000; D_loss: 0.6949620246887207; G_loss: 1.8314661979675293\n",
            "Iter-54000; D_loss: 0.8616863489151001; G_loss: 1.6617070436477661\n",
            "Iter-55000; D_loss: 0.613654613494873; G_loss: 2.101942539215088\n",
            "Iter-56000; D_loss: 0.6808241605758667; G_loss: 2.063056468963623\n",
            "Iter-57000; D_loss: 0.9443448781967163; G_loss: 1.9375745058059692\n",
            "Iter-58000; D_loss: 0.6510494947433472; G_loss: 1.6398730278015137\n",
            "Iter-59000; D_loss: 0.7428879141807556; G_loss: 1.9932832717895508\n",
            "Iter-60000; D_loss: 0.7915565967559814; G_loss: 1.3962271213531494\n",
            "Iter-61000; D_loss: 0.7818509936332703; G_loss: 1.6190818548202515\n",
            "Iter-62000; D_loss: 0.5514201521873474; G_loss: 1.6592785120010376\n",
            "Iter-63000; D_loss: 0.7488338351249695; G_loss: 2.217310905456543\n",
            "Iter-64000; D_loss: 0.7064931392669678; G_loss: 1.4301832914352417\n",
            "Iter-65000; D_loss: 0.6483678817749023; G_loss: 1.5362275838851929\n",
            "Iter-66000; D_loss: 0.9024519920349121; G_loss: 1.879546046257019\n",
            "Iter-67000; D_loss: 0.7959364652633667; G_loss: 1.933075189590454\n",
            "Iter-68000; D_loss: 0.8693708181381226; G_loss: 1.8016868829727173\n",
            "Iter-69000; D_loss: 0.7628142237663269; G_loss: 1.6109018325805664\n",
            "Iter-70000; D_loss: 0.7309181690216064; G_loss: 2.101980209350586\n",
            "Iter-71000; D_loss: 0.8012722730636597; G_loss: 1.8706371784210205\n",
            "Iter-72000; D_loss: 0.6874582767486572; G_loss: 1.6639280319213867\n",
            "Iter-73000; D_loss: 0.6422687768936157; G_loss: 1.5569536685943604\n",
            "Iter-74000; D_loss: 0.8119955062866211; G_loss: 2.075348377227783\n",
            "Iter-75000; D_loss: 0.7681315541267395; G_loss: 1.7867251634597778\n",
            "Iter-76000; D_loss: 0.774082601070404; G_loss: 2.2690398693084717\n",
            "Iter-77000; D_loss: 1.0632805824279785; G_loss: 1.5463522672653198\n",
            "Iter-78000; D_loss: 0.7969443798065186; G_loss: 1.7869176864624023\n",
            "Iter-79000; D_loss: 0.8732521533966064; G_loss: 1.604933500289917\n",
            "Iter-80000; D_loss: 0.6018843650817871; G_loss: 1.9788832664489746\n",
            "Iter-81000; D_loss: 0.5331346988677979; G_loss: 2.0570199489593506\n",
            "Iter-82000; D_loss: 0.9014260768890381; G_loss: 2.2166712284088135\n",
            "Iter-83000; D_loss: 0.8889458179473877; G_loss: 2.2521424293518066\n",
            "Iter-84000; D_loss: 0.7389317750930786; G_loss: 1.5255448818206787\n",
            "Iter-85000; D_loss: 0.8750787973403931; G_loss: 1.6434528827667236\n",
            "Iter-86000; D_loss: 0.9370383024215698; G_loss: 1.8983503580093384\n",
            "Iter-87000; D_loss: 0.7966125011444092; G_loss: 1.882201075553894\n",
            "Iter-88000; D_loss: 0.7746080160140991; G_loss: 1.8116490840911865\n",
            "Iter-89000; D_loss: 0.9597539901733398; G_loss: 1.6682639122009277\n",
            "Iter-90000; D_loss: 0.8449738025665283; G_loss: 1.7666515111923218\n",
            "Iter-91000; D_loss: 0.6864417791366577; G_loss: 2.0307796001434326\n",
            "Iter-92000; D_loss: 0.7743253707885742; G_loss: 1.9929778575897217\n",
            "Iter-93000; D_loss: 0.7557837963104248; G_loss: 1.8360064029693604\n",
            "Iter-94000; D_loss: 0.5743703842163086; G_loss: 2.359046459197998\n",
            "Iter-95000; D_loss: 0.894123911857605; G_loss: 2.0614023208618164\n",
            "Iter-96000; D_loss: 0.8372962474822998; G_loss: 2.096573829650879\n",
            "Iter-97000; D_loss: 0.663982629776001; G_loss: 1.9136954545974731\n",
            "Iter-98000; D_loss: 0.763483464717865; G_loss: 1.8244469165802002\n",
            "Iter-99000; D_loss: 0.8231713771820068; G_loss: 2.0423996448516846\n"
          ]
        }
      ]
    }
  ]
}