{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oWA6qFFUIt1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import tensorflow as tf\n",
        "#from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "# Load MNIST dataset\n",
        "\n",
        "mnist_data = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist_data.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices (one-hot encoding)\n",
        "num_classes = 10 # There are 10 classes in MNIST\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "mb_size = 64\n",
        "Z_dim = 100\n",
        "X_dim = 28*28\n",
        "y_dim = num_classes\n",
        "h_dim = 128\n",
        "cnt = 0\n",
        "lr = 1e-3"
      ],
      "metadata": {
        "id": "aYdhh7MxJHef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6705537c-4903-4993-8fdb-cf7bb4dc23ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
      ],
      "metadata": {
        "id": "F5388LURJ8n6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
        "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
        "\n",
        "\n",
        "def G(z, c):\n",
        "    inputs = torch.cat([z, c], 1)\n",
        "    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
        "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X\n",
        "\n",
        "\n",
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
        "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "\n",
        "def D(X, c):\n",
        "    inputs = torch.cat([X, c], 1)\n",
        "    h = nn.relu(inputs @ Wxh + bxh.repeat(inputs.size(0), 1))\n",
        "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DQR3uhszJ9qb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
        "\n",
        "\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
        "\n",
        "\n",
        "G_solver = optim.Adam(G_params, lr=1e-3)\n",
        "D_solver = optim.Adam(D_params, lr=1e-3)\n",
        "\n",
        "ones_label = Variable(torch.ones(mb_size, 1))\n",
        "zeros_label = Variable(torch.zeros(mb_size, 1))\n",
        "\n",
        "\n",
        "for it in range(100000):\n",
        "    # Sample data\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    idx = np.random.randint(0, X_train.shape[0], mb_size)\n",
        "    X = Variable(torch.from_numpy(X_train[idx]).float())\n",
        "    c = Variable(torch.from_numpy(y_train_one_hot[idx]).float())\n",
        "\n",
        "    # Dicriminator forward-loss-backward-update\n",
        "    G_sample = G(z, c)\n",
        "    D_real = D(X, c)\n",
        "    D_fake = D(G_sample, c)\n",
        "\n",
        "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
        "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    G_sample = G(z, c)\n",
        "    D_fake = D(G_sample, c)\n",
        "\n",
        "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
        "\n",
        "        c = np.zeros(shape=[mb_size, y_dim], dtype='float32')\n",
        "        c[:, np.random.randint(0, 10)] = 1.\n",
        "        #digit = 5  # replace with any number between 0 and 9\n",
        "        #c[:, digit] = 1\n",
        "        c = Variable(torch.from_numpy(c))\n",
        "        samples = G(z, c).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(cnt).zfill(3)), bbox_inches='tight')\n",
        "        cnt += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jcrfCyVKNpr",
        "outputId": "01b2e273-53ac-41ee-85cc-50415a0ea6cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 2.119720935821533; G_loss: 1.4176967144012451\n",
            "Iter-1000; D_loss: 0.0020844005048274994; G_loss: 8.011555671691895\n",
            "Iter-2000; D_loss: 0.01322654727846384; G_loss: 6.9321608543396\n",
            "Iter-3000; D_loss: 0.022163379937410355; G_loss: 6.249037265777588\n",
            "Iter-4000; D_loss: 0.12792082130908966; G_loss: 5.583803176879883\n",
            "Iter-5000; D_loss: 0.12288868427276611; G_loss: 5.131620407104492\n",
            "Iter-6000; D_loss: 0.21928127110004425; G_loss: 4.564157009124756\n",
            "Iter-7000; D_loss: 0.2008344680070877; G_loss: 3.6447410583496094\n",
            "Iter-8000; D_loss: 0.9671663045883179; G_loss: 3.9614453315734863\n",
            "Iter-9000; D_loss: 0.7651663422584534; G_loss: 3.358898639678955\n",
            "Iter-10000; D_loss: 0.6412805318832397; G_loss: 2.8945164680480957\n",
            "Iter-11000; D_loss: 1.0454044342041016; G_loss: 2.365999698638916\n",
            "Iter-12000; D_loss: 1.0522516965866089; G_loss: 2.4185657501220703\n",
            "Iter-13000; D_loss: 0.7694385647773743; G_loss: 2.1334352493286133\n",
            "Iter-14000; D_loss: 0.78901606798172; G_loss: 2.349025249481201\n",
            "Iter-15000; D_loss: 0.7720038294792175; G_loss: 1.612999677658081\n",
            "Iter-16000; D_loss: 0.9790282845497131; G_loss: 2.159837484359741\n",
            "Iter-17000; D_loss: 0.781582772731781; G_loss: 1.9302237033843994\n",
            "Iter-18000; D_loss: 0.9405810832977295; G_loss: 1.8539685010910034\n",
            "Iter-19000; D_loss: 0.77374267578125; G_loss: 1.5439379215240479\n",
            "Iter-20000; D_loss: 0.8781079053878784; G_loss: 2.121769428253174\n",
            "Iter-21000; D_loss: 0.8237701058387756; G_loss: 1.8864502906799316\n",
            "Iter-22000; D_loss: 0.6768486499786377; G_loss: 1.967453122138977\n",
            "Iter-23000; D_loss: 1.012760877609253; G_loss: 1.5812281370162964\n",
            "Iter-24000; D_loss: 1.0789400339126587; G_loss: 1.9803012609481812\n",
            "Iter-25000; D_loss: 0.8349436521530151; G_loss: 1.915587067604065\n",
            "Iter-26000; D_loss: 0.7123720049858093; G_loss: 1.5860896110534668\n",
            "Iter-27000; D_loss: 0.9200857877731323; G_loss: 1.7106610536575317\n",
            "Iter-28000; D_loss: 0.8215943574905396; G_loss: 1.6503338813781738\n",
            "Iter-29000; D_loss: 1.0116240978240967; G_loss: 1.5352206230163574\n",
            "Iter-30000; D_loss: 0.7276874780654907; G_loss: 2.009312629699707\n",
            "Iter-31000; D_loss: 0.8082367181777954; G_loss: 1.8786388635635376\n",
            "Iter-32000; D_loss: 0.7736636400222778; G_loss: 1.689998745918274\n",
            "Iter-33000; D_loss: 1.03744637966156; G_loss: 1.7250885963439941\n",
            "Iter-34000; D_loss: 0.7597821354866028; G_loss: 1.4668445587158203\n",
            "Iter-35000; D_loss: 0.977586030960083; G_loss: 1.5537540912628174\n",
            "Iter-36000; D_loss: 0.8688901662826538; G_loss: 1.6861793994903564\n",
            "Iter-37000; D_loss: 1.0015296936035156; G_loss: 1.8360040187835693\n",
            "Iter-38000; D_loss: 0.7846625447273254; G_loss: 1.6702814102172852\n",
            "Iter-39000; D_loss: 0.940503716468811; G_loss: 1.7718626260757446\n",
            "Iter-40000; D_loss: 0.6992953419685364; G_loss: 1.6729018688201904\n",
            "Iter-41000; D_loss: 0.8105959892272949; G_loss: 1.6003719568252563\n",
            "Iter-42000; D_loss: 1.0738801956176758; G_loss: 1.7902615070343018\n",
            "Iter-43000; D_loss: 0.9726135730743408; G_loss: 1.4764171838760376\n",
            "Iter-44000; D_loss: 0.8488756418228149; G_loss: 1.537769079208374\n",
            "Iter-45000; D_loss: 1.0291948318481445; G_loss: 1.482397437095642\n",
            "Iter-46000; D_loss: 0.9270851612091064; G_loss: 1.417736530303955\n",
            "Iter-47000; D_loss: 1.0163003206253052; G_loss: 1.5466276407241821\n",
            "Iter-48000; D_loss: 1.186640977859497; G_loss: 1.7102805376052856\n",
            "Iter-49000; D_loss: 0.8507492542266846; G_loss: 1.7737256288528442\n",
            "Iter-50000; D_loss: 0.8270235061645508; G_loss: 1.5423812866210938\n",
            "Iter-51000; D_loss: 0.9347332715988159; G_loss: 1.6788499355316162\n",
            "Iter-52000; D_loss: 1.0682692527770996; G_loss: 1.4452548027038574\n",
            "Iter-53000; D_loss: 0.8485475778579712; G_loss: 1.568153977394104\n",
            "Iter-54000; D_loss: 0.8565273284912109; G_loss: 1.4787498712539673\n",
            "Iter-55000; D_loss: 0.7937614917755127; G_loss: 1.5556223392486572\n",
            "Iter-56000; D_loss: 0.7747696042060852; G_loss: 1.4649797677993774\n",
            "Iter-57000; D_loss: 0.8817183375358582; G_loss: 1.6304500102996826\n",
            "Iter-58000; D_loss: 0.9004030227661133; G_loss: 1.6007814407348633\n",
            "Iter-59000; D_loss: 0.8336430788040161; G_loss: 1.5688705444335938\n",
            "Iter-60000; D_loss: 0.9292849898338318; G_loss: 1.6486061811447144\n",
            "Iter-61000; D_loss: 0.8626143932342529; G_loss: 1.8689672946929932\n",
            "Iter-62000; D_loss: 0.8765382766723633; G_loss: 1.7138512134552002\n",
            "Iter-63000; D_loss: 0.9607382416725159; G_loss: 1.6063703298568726\n",
            "Iter-64000; D_loss: 1.036027431488037; G_loss: 1.6140553951263428\n",
            "Iter-65000; D_loss: 1.0167315006256104; G_loss: 1.7029834985733032\n",
            "Iter-66000; D_loss: 0.6315878033638; G_loss: 1.5602400302886963\n",
            "Iter-67000; D_loss: 0.7898050546646118; G_loss: 1.6984726190567017\n",
            "Iter-68000; D_loss: 0.7601697444915771; G_loss: 1.8811854124069214\n",
            "Iter-69000; D_loss: 0.9091688394546509; G_loss: 1.6148375272750854\n",
            "Iter-70000; D_loss: 0.8435527086257935; G_loss: 1.9359607696533203\n",
            "Iter-71000; D_loss: 0.8665727376937866; G_loss: 1.3410019874572754\n",
            "Iter-72000; D_loss: 0.866784930229187; G_loss: 1.6219513416290283\n",
            "Iter-73000; D_loss: 0.8245742917060852; G_loss: 1.3954828977584839\n",
            "Iter-74000; D_loss: 1.008284330368042; G_loss: 1.6331841945648193\n",
            "Iter-75000; D_loss: 0.9209685921669006; G_loss: 1.6601202487945557\n",
            "Iter-76000; D_loss: 0.8935102224349976; G_loss: 1.5434359312057495\n",
            "Iter-77000; D_loss: 0.948261022567749; G_loss: 1.7896384000778198\n",
            "Iter-78000; D_loss: 0.9097036123275757; G_loss: 1.641189455986023\n",
            "Iter-79000; D_loss: 0.9283660650253296; G_loss: 1.6128815412521362\n",
            "Iter-80000; D_loss: 0.8007930517196655; G_loss: 1.7828837633132935\n",
            "Iter-81000; D_loss: 0.7726856470108032; G_loss: 1.4861680269241333\n",
            "Iter-82000; D_loss: 0.8798855543136597; G_loss: 1.8200507164001465\n",
            "Iter-83000; D_loss: 0.7675796747207642; G_loss: 1.8299332857131958\n",
            "Iter-84000; D_loss: 0.8999147415161133; G_loss: 1.6806650161743164\n",
            "Iter-85000; D_loss: 0.8052586913108826; G_loss: 1.5818164348602295\n",
            "Iter-86000; D_loss: 0.9927278161048889; G_loss: 1.9142159223556519\n",
            "Iter-87000; D_loss: 1.0153793096542358; G_loss: 1.2192423343658447\n",
            "Iter-88000; D_loss: 0.787837028503418; G_loss: 1.5785503387451172\n",
            "Iter-89000; D_loss: 0.8877356052398682; G_loss: 1.884400486946106\n",
            "Iter-90000; D_loss: 0.9725416898727417; G_loss: 1.9988923072814941\n",
            "Iter-91000; D_loss: 0.7888664603233337; G_loss: 1.6260690689086914\n",
            "Iter-92000; D_loss: 0.8266191482543945; G_loss: 1.6670851707458496\n",
            "Iter-93000; D_loss: 0.8110289573669434; G_loss: 1.672371506690979\n",
            "Iter-94000; D_loss: 0.7129665613174438; G_loss: 1.7584898471832275\n",
            "Iter-95000; D_loss: 0.8209763765335083; G_loss: 1.8080589771270752\n",
            "Iter-96000; D_loss: 0.9949248433113098; G_loss: 1.792039394378662\n",
            "Iter-97000; D_loss: 0.8291473984718323; G_loss: 1.9289582967758179\n",
            "Iter-98000; D_loss: 0.9347639679908752; G_loss: 1.5249346494674683\n",
            "Iter-99000; D_loss: 0.6813501119613647; G_loss: 1.4456310272216797\n"
          ]
        }
      ]
    }
  ]
}