{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5E7xJY1urkw"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch numpy matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRzOZUSrtNq0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "#from tensorflow.examples.tutorials.mnist import input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94KBKRehujS1",
        "outputId": "1833a594-0486-4492-fd33-82e093129d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:04<00:00, 2262225.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 339826.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1263309.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8282838.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_trainset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_testset, batch_size=64, shuffle=False)\n",
        "\n",
        "mb_size = 64\n",
        "Z_dim = 100\n",
        "X_dim = mnist_trainset[0][0].shape[1] * mnist_trainset[0][0].shape[2]  # should be 28*28 = 784\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6f24qrsthUr"
      },
      "outputs": [],
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / torch.sqrt(torch.tensor(in_dim / 2., device=device))\n",
        "    return torch.nn.Parameter(torch.randn(*size, device=device) * xavier_stddev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjoWye5myRXa"
      },
      "outputs": [],
      "source": [
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
        "bzh = torch.nn.Parameter(torch.zeros(h_dim, device=device))\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = torch.nn.Parameter(torch.zeros(X_dim, device=device))\n",
        "\n",
        "def G(z):\n",
        "    h = F.relu(torch.mm(z, Wzh) + bzh)\n",
        "    X = torch.sigmoid(torch.mm(h, Whx) + bhx)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8cJNqQ2y_8C"
      },
      "outputs": [],
      "source": [
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim, h_dim])\n",
        "bxh = torch.nn.Parameter(torch.zeros(h_dim, device=device))\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = torch.nn.Parameter(torch.zeros(1, device=device))\n",
        "\n",
        "def D(X):\n",
        "    h = F.relu(torch.mm(X.view(X.size(0), -1), Wxh) + bxh)\n",
        "    y = torch.sigmoid(torch.mm(h, Why) + bhy)\n",
        "    return y\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nE0hGsry9Vd",
        "outputId": "eb22df69-0f96-470f-eb29-b7585042c3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 0.0033271959982812405; G_loss: 8.435750007629395\n",
            "Iter-100; D_loss: 0.6213933229446411; G_loss: 2.1821322441101074\n",
            "Iter-200; D_loss: 0.6876852512359619; G_loss: 2.3392324447631836\n",
            "Iter-300; D_loss: 0.49598371982574463; G_loss: 2.7474308013916016\n",
            "Iter-400; D_loss: 0.23141580820083618; G_loss: 3.424255847930908\n",
            "Iter-500; D_loss: 0.31467732787132263; G_loss: 2.4547605514526367\n",
            "Iter-600; D_loss: 0.8529813289642334; G_loss: 3.2991485595703125\n"
          ]
        }
      ],
      "source": [
        "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
        "\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            p.grad.data.zero_()\n",
        "\n",
        "G_solver = optim.Adam([Wzh, bzh, Whx, bhx], lr=lr)\n",
        "D_solver = optim.Adam([Wxh, bxh, Why, bhy], lr=lr)\n",
        "\n",
        "for it in range(10000):\n",
        "    for X, _ in train_loader:\n",
        "        current_batch_size = X.size(0)\n",
        "\n",
        "        z = torch.randn(current_batch_size, Z_dim, device=device)\n",
        "        ones_label = torch.ones(current_batch_size, 1, device=device)\n",
        "        zeros_label = torch.zeros(current_batch_size, 1, device=device)\n",
        "        X = X.to(device)\n",
        "\n",
        "        # Discriminator forward-loss-backward-update\n",
        "        G_sample = G(z)\n",
        "        D_real = D(X)\n",
        "        D_fake = D(G_sample)\n",
        "\n",
        "        D_loss_real = F.binary_cross_entropy(D_real, ones_label)\n",
        "        D_loss_fake = F.binary_cross_entropy(D_fake, zeros_label)\n",
        "        D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "        D_solver.zero_grad()\n",
        "        D_loss.backward()\n",
        "        D_solver.step()\n",
        "\n",
        "        # Generator forward-loss-backward-update\n",
        "        z = torch.randn(current_batch_size, Z_dim, device=device)\n",
        "        G_sample = G(z)\n",
        "        D_fake = D(G_sample)\n",
        "\n",
        "        G_loss = F.binary_cross_entropy(D_fake, ones_label)\n",
        "\n",
        "        G_solver.zero_grad()\n",
        "        G_loss.backward()\n",
        "        G_solver.step()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 100 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.item(), G_loss.item()))\n",
        "\n",
        "        samples = G(z).detach().cpu().numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}