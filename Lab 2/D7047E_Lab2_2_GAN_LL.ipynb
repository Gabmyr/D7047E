{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5E7xJY1urkw"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch numpy matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LRzOZUSrtNq0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import tensorflow as tf\n",
        "#from torchvision import datasets, transforms\n",
        "#from tensorflow.examples.tutorials.mnist import input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94KBKRehujS1",
        "outputId": "2750a523-d198-416b-a210-246a3b473dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# Load MNIST dataset\n",
        "\n",
        "mnist_data = tf.keras.datasets.mnist\n",
        "(X_train,_),(X_test,_) = mnist_data.load_data()\n",
        "\n",
        "train_loader = X_train.reshape(-1,28*28) / 255.0\n",
        "test_loader = X_test.reshape(-1,28*28) / 255.0\n",
        "\n",
        "mb_size = 64\n",
        "Z_dim = 100\n",
        "X_dim = 28*28\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j6f24qrsthUr"
      },
      "outputs": [],
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / torch.sqrt(torch.tensor(in_dim / 2., device=device))\n",
        "    return torch.nn.Parameter(torch.randn(*size, device=device) * xavier_stddev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FjoWye5myRXa"
      },
      "outputs": [],
      "source": [
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
        "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
        "\n",
        "def G(z):\n",
        "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
        "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P8cJNqQ2y_8C"
      },
      "outputs": [],
      "source": [
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim, h_dim])\n",
        "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "def D(X):\n",
        "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
        "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ===================== 2.2 TRAINING logistic loss 20K/100K======================== \"\"\"\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
        "\n",
        "\n",
        "G_solver = optim.Adam(G_params, lr=1e-3)\n",
        "D_solver = optim.Adam(D_params, lr=1e-3)\n",
        "\n",
        "ones_label = Variable(torch.ones(mb_size, 1))\n",
        "zeros_label = Variable(torch.zeros(mb_size, 1))\n",
        "\n",
        "\n",
        "for it in range(100000):\n",
        "    # Sample data\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    idx = np.random.randint(0, train_loader.shape[0], mb_size)\n",
        "    X = Variable(torch.from_numpy(train_loader[idx]).float())\n",
        "\n",
        "    # Dicriminator forward-loss-backward-update\n",
        "    G_sample = G(z)\n",
        "    D_real = D(X)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
        "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    G_sample = G(z)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
        "\n",
        "        samples = G(z).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whf0y-_FR_Ww",
        "outputId": "3ffdfc3a-55ae-4c38-9373-bc8266efab8d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 0.614266574382782; G_loss: 2.2302069664001465\n",
            "Iter-1000; D_loss: 0.7152239680290222; G_loss: 2.1338486671447754\n",
            "Iter-2000; D_loss: 0.5606213808059692; G_loss: 2.1985132694244385\n",
            "Iter-3000; D_loss: 0.6654432415962219; G_loss: 2.538466691970825\n",
            "Iter-4000; D_loss: 0.39272552728652954; G_loss: 2.7692525386810303\n",
            "Iter-5000; D_loss: 0.5662136673927307; G_loss: 2.278355121612549\n",
            "Iter-6000; D_loss: 0.42721202969551086; G_loss: 2.560878038406372\n",
            "Iter-7000; D_loss: 0.5624514818191528; G_loss: 2.361584424972534\n",
            "Iter-8000; D_loss: 0.47946757078170776; G_loss: 2.465975046157837\n",
            "Iter-9000; D_loss: 0.6758779883384705; G_loss: 2.2267396450042725\n",
            "Iter-10000; D_loss: 0.6571389436721802; G_loss: 2.432741165161133\n",
            "Iter-11000; D_loss: 0.5256248712539673; G_loss: 2.5864264965057373\n",
            "Iter-12000; D_loss: 0.48997095227241516; G_loss: 2.630333185195923\n",
            "Iter-13000; D_loss: 0.446507066488266; G_loss: 2.7108314037323\n",
            "Iter-14000; D_loss: 0.4788537323474884; G_loss: 2.591122627258301\n",
            "Iter-15000; D_loss: 0.45209550857543945; G_loss: 2.5051429271698\n",
            "Iter-16000; D_loss: 0.6436355113983154; G_loss: 2.5501880645751953\n",
            "Iter-17000; D_loss: 0.6147966384887695; G_loss: 2.634859800338745\n",
            "Iter-18000; D_loss: 0.4853360652923584; G_loss: 2.726407766342163\n",
            "Iter-19000; D_loss: 0.5459977388381958; G_loss: 2.7564852237701416\n",
            "Iter-20000; D_loss: 0.48088538646698; G_loss: 2.8413941860198975\n",
            "Iter-21000; D_loss: 0.5277274250984192; G_loss: 2.646209478378296\n",
            "Iter-22000; D_loss: 0.6111137270927429; G_loss: 2.556936502456665\n",
            "Iter-23000; D_loss: 0.5177880525588989; G_loss: 2.3172104358673096\n",
            "Iter-24000; D_loss: 0.48466888070106506; G_loss: 2.362891674041748\n",
            "Iter-25000; D_loss: 0.8072906732559204; G_loss: 2.702542304992676\n",
            "Iter-26000; D_loss: 0.5532369613647461; G_loss: 2.673701286315918\n",
            "Iter-27000; D_loss: 0.557535707950592; G_loss: 2.831883192062378\n",
            "Iter-28000; D_loss: 0.718041718006134; G_loss: 2.3308746814727783\n",
            "Iter-29000; D_loss: 0.48783057928085327; G_loss: 2.9568545818328857\n",
            "Iter-30000; D_loss: 0.6808348894119263; G_loss: 2.580883502960205\n",
            "Iter-31000; D_loss: 0.3735470175743103; G_loss: 2.7120463848114014\n",
            "Iter-32000; D_loss: 0.4489319920539856; G_loss: 2.031040668487549\n",
            "Iter-33000; D_loss: 0.612530529499054; G_loss: 2.306337833404541\n",
            "Iter-34000; D_loss: 0.4827502369880676; G_loss: 2.4190006256103516\n",
            "Iter-35000; D_loss: 0.5461968183517456; G_loss: 2.357619047164917\n",
            "Iter-36000; D_loss: 0.5716829299926758; G_loss: 2.286580801010132\n",
            "Iter-37000; D_loss: 0.5677568316459656; G_loss: 2.502856731414795\n",
            "Iter-38000; D_loss: 0.7226687669754028; G_loss: 2.4298171997070312\n",
            "Iter-39000; D_loss: 0.7882047891616821; G_loss: 2.433419704437256\n",
            "Iter-40000; D_loss: 0.417702853679657; G_loss: 2.477058172225952\n",
            "Iter-41000; D_loss: 0.5722522139549255; G_loss: 2.7338907718658447\n",
            "Iter-42000; D_loss: 0.6762508153915405; G_loss: 3.1635618209838867\n",
            "Iter-43000; D_loss: 0.4233434200286865; G_loss: 2.5677897930145264\n",
            "Iter-44000; D_loss: 0.6071261167526245; G_loss: 2.7283315658569336\n",
            "Iter-45000; D_loss: 0.49764928221702576; G_loss: 2.555039167404175\n",
            "Iter-46000; D_loss: 0.4344857633113861; G_loss: 2.6979053020477295\n",
            "Iter-47000; D_loss: 0.47147512435913086; G_loss: 2.761993169784546\n",
            "Iter-48000; D_loss: 0.5913097262382507; G_loss: 2.2846603393554688\n",
            "Iter-49000; D_loss: 0.49055829644203186; G_loss: 2.4765563011169434\n",
            "Iter-50000; D_loss: 0.6684522032737732; G_loss: 2.6599433422088623\n",
            "Iter-51000; D_loss: 0.623067319393158; G_loss: 2.5721912384033203\n",
            "Iter-52000; D_loss: 0.5155063271522522; G_loss: 2.5518341064453125\n",
            "Iter-53000; D_loss: 0.5007924437522888; G_loss: 2.784799098968506\n",
            "Iter-54000; D_loss: 0.5128031373023987; G_loss: 3.223726749420166\n",
            "Iter-55000; D_loss: 0.48993057012557983; G_loss: 2.5402865409851074\n",
            "Iter-56000; D_loss: 0.4129512310028076; G_loss: 2.5959699153900146\n",
            "Iter-57000; D_loss: 0.5310682654380798; G_loss: 2.885383367538452\n",
            "Iter-58000; D_loss: 0.42227160930633545; G_loss: 2.8012237548828125\n",
            "Iter-59000; D_loss: 0.6718135476112366; G_loss: 2.2499215602874756\n",
            "Iter-60000; D_loss: 0.3908021152019501; G_loss: 2.7703359127044678\n",
            "Iter-61000; D_loss: 0.4013613760471344; G_loss: 2.533690929412842\n",
            "Iter-62000; D_loss: 0.4775826334953308; G_loss: 2.6921677589416504\n",
            "Iter-63000; D_loss: 0.5852110385894775; G_loss: 2.63128399848938\n",
            "Iter-64000; D_loss: 0.5900838375091553; G_loss: 2.94683837890625\n",
            "Iter-65000; D_loss: 0.3317868113517761; G_loss: 3.5046277046203613\n",
            "Iter-66000; D_loss: 0.48625296354293823; G_loss: 2.9888827800750732\n",
            "Iter-67000; D_loss: 0.42166250944137573; G_loss: 2.4880542755126953\n",
            "Iter-68000; D_loss: 0.4407711625099182; G_loss: 2.4125277996063232\n",
            "Iter-69000; D_loss: 0.45334136486053467; G_loss: 3.0189943313598633\n",
            "Iter-70000; D_loss: 0.49753862619400024; G_loss: 3.046830177307129\n",
            "Iter-71000; D_loss: 0.539115309715271; G_loss: 3.1394858360290527\n",
            "Iter-72000; D_loss: 0.5357086062431335; G_loss: 2.7612459659576416\n",
            "Iter-73000; D_loss: 0.5106903314590454; G_loss: 2.832690954208374\n",
            "Iter-74000; D_loss: 0.6381949186325073; G_loss: 3.3605599403381348\n",
            "Iter-75000; D_loss: 0.44759446382522583; G_loss: 2.8185667991638184\n",
            "Iter-76000; D_loss: 0.47369158267974854; G_loss: 2.802041530609131\n",
            "Iter-77000; D_loss: 0.39134806394577026; G_loss: 2.695300340652466\n",
            "Iter-78000; D_loss: 0.4818238615989685; G_loss: 2.429278612136841\n",
            "Iter-79000; D_loss: 0.2778306305408478; G_loss: 2.7304563522338867\n",
            "Iter-80000; D_loss: 0.4966776669025421; G_loss: 2.892150402069092\n",
            "Iter-81000; D_loss: 0.5093440413475037; G_loss: 3.1759605407714844\n",
            "Iter-82000; D_loss: 0.5561142563819885; G_loss: 2.8410353660583496\n",
            "Iter-83000; D_loss: 0.35740119218826294; G_loss: 2.7368316650390625\n",
            "Iter-84000; D_loss: 0.41491588950157166; G_loss: 2.869769811630249\n",
            "Iter-85000; D_loss: 0.43731385469436646; G_loss: 2.3981895446777344\n",
            "Iter-86000; D_loss: 0.5476366281509399; G_loss: 2.725166082382202\n",
            "Iter-87000; D_loss: 0.6982078552246094; G_loss: 3.0267562866210938\n",
            "Iter-88000; D_loss: 0.5298720598220825; G_loss: 3.1187844276428223\n",
            "Iter-89000; D_loss: 0.3634220063686371; G_loss: 2.7685773372650146\n",
            "Iter-90000; D_loss: 0.5235568881034851; G_loss: 2.7702865600585938\n",
            "Iter-91000; D_loss: 0.46250417828559875; G_loss: 2.7102110385894775\n",
            "Iter-92000; D_loss: 0.40972381830215454; G_loss: 2.9364945888519287\n",
            "Iter-93000; D_loss: 0.4909391403198242; G_loss: 3.3508453369140625\n",
            "Iter-94000; D_loss: 0.5224792957305908; G_loss: 3.2898194789886475\n",
            "Iter-95000; D_loss: 0.517760157585144; G_loss: 2.699469566345215\n",
            "Iter-96000; D_loss: 0.4796392321586609; G_loss: 2.950593948364258\n",
            "Iter-97000; D_loss: 0.6865519285202026; G_loss: 2.1717798709869385\n",
            "Iter-98000; D_loss: 0.4324601888656616; G_loss: 2.9098479747772217\n",
            "Iter-99000; D_loss: 0.3649706244468689; G_loss: 2.8399147987365723\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}