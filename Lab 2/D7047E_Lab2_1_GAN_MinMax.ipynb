{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os"
      ],
      "metadata": {
        "id": "QjZU5JK4ctEE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, _), (_, _) = mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], -1).astype('float32')\n",
        "train_images = (train_images) / 255  # Normalize the images to [-1, 1]"
      ],
      "metadata": {
        "id": "KwRVT2VtcvLg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xavier Initialization\n",
        "def xavier_init(shape, dtype=tf.dtypes.float32):\n",
        "    in_dim = shape[0]\n",
        "    xavier_stddev = 1. / tf.math.sqrt(in_dim / 2.)\n",
        "    return tf.random.normal(shape=shape, stddev=xavier_stddev, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "# Generator\n",
        "def generator(z):\n",
        "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
        "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
        "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
        "    return G_prob\n",
        "\n",
        "# Discriminator\n",
        "def discriminator(x):\n",
        "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
        "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
        "    D_prob = tf.nn.sigmoid(D_logit)\n",
        "    return D_prob, D_logit\n",
        "\n",
        "# Discriminator Loss  - minmax loss\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return -tf.reduce_mean(tf.math.log(real_output + 1e-8) + tf.math.log(1. - fake_output + 1e-8))\n",
        "\n",
        "# Generator Loss  minmax loss\n",
        "def generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(tf.math.log(fake_output + 1e-8))\n",
        "\n",
        "# Optimizers\n",
        "D_optimizer = tf.optimizers.Adam(1e-4)\n",
        "G_optimizer = tf.optimizers.Adam(1e-4)\n",
        "\n",
        "# Training step\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = sample_Z(mb_size, Z_dim)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        G_sample = generator(noise)\n",
        "        D_real, _ = discriminator(images)\n",
        "        D_fake, _ = discriminator(G_sample)\n",
        "\n",
        "        D_loss = discriminator_loss(D_real, D_fake)\n",
        "        G_loss = generator_loss(D_fake)\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(D_loss, theta_D)\n",
        "    gradients_of_generator = gen_tape.gradient(G_loss, theta_G)\n",
        "\n",
        "    D_optimizer.apply_gradients(zip(gradients_of_discriminator, theta_D))\n",
        "    G_optimizer.apply_gradients(zip(gradients_of_generator, theta_G))\n",
        "\n",
        "def sample_Z(m, n):\n",
        "    return np.random.uniform(-1., 1., size=[m, n]).astype(np.float32)  # Cast to float32\n",
        "\n",
        "# Plotting function\n",
        "def plot(samples):\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    gs = gridspec.GridSpec(4, 4)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Training step\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = sample_Z(mb_size, Z_dim)\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        G_sample = generator(noise)\n",
        "        D_real, _ = discriminator(images)\n",
        "        D_fake, _ = discriminator(G_sample)\n",
        "\n",
        "        D_loss = discriminator_loss(D_real, D_fake)\n",
        "        G_loss = generator_loss(D_fake)\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(D_loss, theta_D)\n",
        "    gradients_of_generator = gen_tape.gradient(G_loss, theta_G)\n",
        "\n",
        "    D_optimizer.apply_gradients(zip(gradients_of_discriminator, theta_D))\n",
        "    G_optimizer.apply_gradients(zip(gradients_of_generator, theta_G))\n",
        "    return D_loss, G_loss"
      ],
      "metadata": {
        "id": "hRoDwC9Ac6xs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "mb_size = 128\n",
        "Z_dim = 100\n",
        "\n",
        "# Discriminator Weights and Biases\n",
        "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
        "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
        "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
        "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
        "theta_D = [D_W1, D_b1, D_W2, D_b2]\n",
        "\n",
        "# Generator Weights and Biases\n",
        "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
        "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
        "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
        "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
        "theta_G = [G_W1, G_b1, G_W2, G_b2]"
      ],
      "metadata": {
        "id": "FZ-gDPykc1gU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYGZfqS2VEir",
        "outputId": "d66e6a70-0b73-4c6f-dc52-5128a2acbf38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 1.3915553092956543; G_loss: 1.5876109600067139\n",
            "Iter-1000; D_loss: 0.36768266558647156; G_loss: 2.3177099227905273\n",
            "Iter-2000; D_loss: 0.3169485926628113; G_loss: 1.947006344795227\n",
            "Iter-3000; D_loss: 0.4201398193836212; G_loss: 1.4058104753494263\n",
            "Iter-4000; D_loss: 0.406434029340744; G_loss: 1.597029447555542\n",
            "Iter-5000; D_loss: 0.4634042978286743; G_loss: 1.9123272895812988\n",
            "Iter-6000; D_loss: 0.5394510626792908; G_loss: 1.6409804821014404\n",
            "Iter-7000; D_loss: 0.46160179376602173; G_loss: 1.8026102781295776\n",
            "Iter-8000; D_loss: 0.5959134101867676; G_loss: 1.6496095657348633\n",
            "Iter-9000; D_loss: 0.7266929149627686; G_loss: 1.5127217769622803\n",
            "Iter-10000; D_loss: 0.6457779407501221; G_loss: 1.572351336479187\n",
            "Iter-11000; D_loss: 0.6271353960037231; G_loss: 1.529844880104065\n",
            "Iter-12000; D_loss: 0.6523762941360474; G_loss: 1.5388013124465942\n",
            "Iter-13000; D_loss: 0.6657979488372803; G_loss: 1.5472614765167236\n",
            "Iter-14000; D_loss: 0.6550084352493286; G_loss: 1.4824557304382324\n",
            "Iter-15000; D_loss: 0.7388581037521362; G_loss: 1.4203541278839111\n",
            "Iter-16000; D_loss: 0.7368484139442444; G_loss: 1.4714018106460571\n",
            "Iter-17000; D_loss: 0.7966400384902954; G_loss: 1.3686332702636719\n",
            "Iter-18000; D_loss: 0.8077447414398193; G_loss: 1.3918429613113403\n",
            "Iter-19000; D_loss: 0.7751774787902832; G_loss: 1.347150444984436\n",
            "Iter-20000; D_loss: 0.8794751167297363; G_loss: 1.3691705465316772\n",
            "Iter-21000; D_loss: 1.0280945301055908; G_loss: 1.2504255771636963\n",
            "Iter-22000; D_loss: 0.9542272686958313; G_loss: 1.316062331199646\n",
            "Iter-23000; D_loss: 1.0375152826309204; G_loss: 1.3376801013946533\n",
            "Iter-24000; D_loss: 0.9467968940734863; G_loss: 1.3871965408325195\n",
            "Iter-25000; D_loss: 0.9524218440055847; G_loss: 1.2909741401672363\n",
            "Iter-26000; D_loss: 0.9983310699462891; G_loss: 1.2776050567626953\n",
            "Iter-27000; D_loss: 0.9217917919158936; G_loss: 1.3014650344848633\n",
            "Iter-28000; D_loss: 0.9974120855331421; G_loss: 1.3485372066497803\n",
            "Iter-29000; D_loss: 0.9153727293014526; G_loss: 1.2054331302642822\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "if not os.path.exists('out/'):\n",
        "    os.makedirs('out/')\n",
        "\n",
        "i = 0\n",
        "for it in range(30000):\n",
        "    batch_index = np.random.randint(0, train_images.shape[0], mb_size)\n",
        "    X_mb = train_images[batch_index]\n",
        "\n",
        "    D_loss_curr, G_loss_curr = train_step(X_mb)  # Capture the loss values here\n",
        "\n",
        "    if it % 1000 == 0:\n",
        "        G_sample = generator(sample_Z(16, Z_dim))\n",
        "        samples = G_sample.numpy()\n",
        "        fig = plot(samples)\n",
        "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
        "        i += 1\n",
        "        plt.close(fig)\n",
        "\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss_curr.numpy(), G_loss_curr.numpy()))\n"
      ]
    }
  ]
}